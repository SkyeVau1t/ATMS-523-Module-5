{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f25198a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw columns in CSV: ['Zh (dBZ)', 'Zdr (dB)', 'Ldr (dB)', 'Kdp (deg km-1)', 'Ah (dBZ/km)', 'Adr (dB/km)', 'R (mm/hr)']\n",
      "Matched columns: {'Zh': 'Zh (dBZ)', 'Zdr': 'Zdr (dB)', 'Ldr': 'Ldr (dB)', 'Kdp': 'Kdp (deg km-1)', 'Ah': 'Ah (dBZ/km)', 'Adp': 'Adr (dB/km)', 'R': 'R (mm/hr)'}\n",
      "            Baseline Z–R | R^2 train:  0.2756 | R^2 test:  0.3566 | RMSE train:    7.1439 | RMSE test:    7.1893\n",
      "       Linear Regression | R^2 train:  0.9879 | R^2 test:  0.9891 | RMSE train:    0.9229 | RMSE test:    0.9358\n",
      "Fitting 7 folds for each of 10 candidates, totalling 70 fits\n",
      "      Polynomial (deg=3) | R^2 train:  0.9980 | R^2 test:  0.9980 | RMSE train:    0.3742 | RMSE test:    0.4054\n",
      "Fitting 7 folds for each of 64 candidates, totalling 448 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tnaut\\anaconda3\\envs\\xarray-climate\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning: \n",
      "14 fits failed out of a total of 448.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "14 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tnaut\\anaconda3\\envs\\xarray-climate\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tnaut\\anaconda3\\envs\\xarray-climate\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\tnaut\\anaconda3\\envs\\xarray-climate\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 486, in fit\n",
      "    trees = Parallel(\n",
      "    ...<2 lines>...\n",
      "        prefer=\"threads\",\n",
      "    )(\n",
      "        delayed(_parallel_build_trees)(\n",
      "    ...<12 lines>...\n",
      "        for i, t in enumerate(trees)\n",
      "    )\n",
      "  File \"c:\\Users\\tnaut\\anaconda3\\envs\\xarray-climate\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 82, in __call__\n",
      "    return super().__call__(iterable_with_config_and_warning_filters)\n",
      "           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tnaut\\anaconda3\\envs\\xarray-climate\\Lib\\site-packages\\joblib\\parallel.py\", line 1986, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ~~~~^^^^^^^^\n",
      "  File \"c:\\Users\\tnaut\\anaconda3\\envs\\xarray-climate\\Lib\\site-packages\\joblib\\parallel.py\", line 1914, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"c:\\Users\\tnaut\\anaconda3\\envs\\xarray-climate\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 147, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tnaut\\anaconda3\\envs\\xarray-climate\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 196, in _parallel_build_trees\n",
      "    tree._fit(\n",
      "    ~~~~~~~~~^\n",
      "        X,\n",
      "        ^^\n",
      "    ...<3 lines>...\n",
      "        missing_values_in_feature_mask=missing_values_in_feature_mask,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\tnaut\\anaconda3\\envs\\xarray-climate\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 472, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"sklearn/tree/_tree.pyx\", line 141, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn/tree/_tree.pyx\", line 256, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn/tree/_tree.pyx\", line 911, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn/tree/_tree.pyx\", line 879, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn/tree/_utils.pyx\", line 29, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 2097152 bytes\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\tnaut\\anaconda3\\envs\\xarray-climate\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [0.97386081 0.97428085 0.96630566 0.96697581 0.96456955 0.96504889\n",
      " 0.96292049 0.96296059 0.98214934 0.98194867 0.9756332  0.97573268\n",
      " 0.97419174 0.97469911 0.97218713 0.97293144 0.97588309 0.9758169\n",
      " 0.96729446 0.96726109 0.96522961 0.9655586  0.96346311 0.96345358\n",
      " 0.98335392 0.98317019 0.97625988 0.97643502 0.9748587  0.97539888\n",
      " 0.97289688 0.97349554 0.9768914  0.9774574  0.97428062 0.97451006\n",
      " 0.97227071 0.97235756 0.97051264 0.97067161 0.97598655 0.97595525\n",
      " 0.97111629 0.97117087 0.96832497 0.96832283 0.96777221 0.96777414\n",
      " 0.97955259        nan 0.97564961 0.97606348 0.97297721 0.97359798\n",
      " 0.97251446 0.97221745 0.97772839        nan 0.97301318 0.97300731\n",
      " 0.96987903 0.96984316 0.96922747 0.96924158]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Random Forest (tuned) | R^2 train:  0.9974 | R^2 test:  0.9879 | RMSE train:    0.4268 | RMSE test:    0.9861\n",
      "\n",
      "=== Summary ===\n",
      "                model  r2_train  r2_test  rmse_train  rmse_test\n",
      "         Baseline Z–R  0.275551 0.356643    7.143950   7.189316\n",
      "    Linear Regression  0.987909 0.989099    0.922940   0.935812\n",
      "   Polynomial (deg=3)  0.998013 0.997954    0.374167   0.405398\n",
      "Random Forest (tuned)  0.997415 0.987897    0.426760   0.986075\n",
      "\n",
      "Best Polynomial degree: 3\n",
      "Best RF params: {'bootstrap': True, 'max_depth': 100, 'max_features': 1.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# homework/module5_project.py\n",
    "# ATMS 523 - Module 5 Project\n",
    "# Robust loader + Baseline + Linear + Polynomial CV + Tuned Random Forest (Windows-friendly)\n",
    "# Outputs a summary table and saves it to homework/module5_results_summary.csv\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from joblib import parallel_backend\n",
    "\n",
    "\n",
    "# ----------------------------- Helpers -----------------------------\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def report(name, y_tr, yhat_tr, y_te, yhat_te):\n",
    "    r2_tr, r2_te = r2_score(y_tr, yhat_tr), r2_score(y_te, yhat_te)\n",
    "    rmse_tr, rmse_te = rmse(y_tr, yhat_tr), rmse(y_te, yhat_te)\n",
    "    print(f\"{name:>24} | R^2 train: {r2_tr:7.4f} | R^2 test: {r2_te:7.4f} | \"\n",
    "          f\"RMSE train: {rmse_tr:9.4f} | RMSE test: {rmse_te:9.4f}\")\n",
    "    return {\"model\": name, \"r2_train\": r2_tr, \"r2_test\": r2_te,\n",
    "            \"rmse_train\": rmse_tr, \"rmse_test\": rmse_te}\n",
    "\n",
    "def baseline_R_from_Zh_dBZ(zh_dbz_series: pd.Series) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Baseline Z–R relation:\n",
    "        Z = 200 * R^1.6, with Z = 10^(dBZ/10)\n",
    "        => R_hat = ((10^(dBZ/10)) / 200)^(1/1.6)\n",
    "    \"\"\"\n",
    "    Z_linear = 10 ** (zh_dbz_series.values / 10.0)\n",
    "    R_hat = (Z_linear / 200.0) ** (1.0 / 1.6)\n",
    "    return R_hat\n",
    "\n",
    "def norm_key(s: str) -> str:\n",
    "    \"\"\"Normalize header text for flexible matching.\"\"\"\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r\"\\(.*?\\)\", \"\", s)          # remove units in parentheses\n",
    "    s = s.replace(\"%\", \"\")\n",
    "    s = re.sub(r\"[\\s_\\-\\/]+\", \"\", s)       # remove spaces/underscores/hyphens/slashes\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9]\", \"\", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "# ----------------------------- Load Data (Robust) -----------------------------\n",
    "DATA_PATH = Path(\"homework/radar_parameters.csv\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Drop obvious auto-index column if present\n",
    "df = df.drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
    "\n",
    "print(\"Raw columns in CSV:\", list(df.columns))\n",
    "\n",
    "# Build normalized lookup for present columns\n",
    "present_norm = {norm_key(c): c for c in df.columns}\n",
    "\n",
    "# Map common variants to canonical names\n",
    "alias_map = {\n",
    "    \"Zh\":  [\"zh\", \"z\", \"dbz\", \"zhh\", \"zhhh\", \"reflectivity\", \"z_h\", \"z_hh\", \"zhdbz\"],\n",
    "    \"Zdr\": [\"zdr\", \"zdrdb\", \"z_dr\"],\n",
    "    \"Ldr\": [\"ldr\", \"l_dr\"],\n",
    "    \"Kdp\": [\"kdp\", \"k_dp\", \"kdpdegkm1\", \"kdpdegkm\"],\n",
    "    \"Ah\":  [\"ah\", \"a_h\", \"attn\", \"specificattenuation\", \"ahh\", \"specattn\", \"ahdbzkm\"],\n",
    "    # Treat ADR as ADP for this assignment dataset\n",
    "    \"Adp\": [\"adp\", \"a_dp\", \"diffattenuation\", \"diffattn\", \"adr\", \"a_dr\", \"adrdbkm\", \"adpdbkm\"],\n",
    "    \"R\":   [\"r\", \"rainrate\", \"rain_rate\", \"rr\", \"rain\", \"preciprate\", \"rmmhr\"],\n",
    "}\n",
    "\n",
    "# Choose the best matching column for each canonical name\n",
    "chosen = {}\n",
    "for canon, variants in alias_map.items():\n",
    "    found = None\n",
    "    for v in variants:\n",
    "        if v in present_norm:\n",
    "            found = present_norm[v]\n",
    "            break\n",
    "    if not found and canon in df.columns:\n",
    "        found = canon\n",
    "    if found:\n",
    "        chosen[canon] = found\n",
    "\n",
    "print(\"Matched columns:\", chosen)\n",
    "\n",
    "required = [\"Zh\", \"Zdr\", \"Ldr\", \"Kdp\", \"Ah\", \"Adp\", \"R\"]\n",
    "missing = [c for c in required if c not in chosen]\n",
    "if missing:\n",
    "    raise KeyError(\n",
    "        \"Could not find these required columns in the CSV after normalization: \"\n",
    "        f\"{missing}\\nRaw columns were: {list(df.columns)}\\n\"\n",
    "        \"If your file truly lacks a column, remove it from `required` & `X` below \"\n",
    "        \"or update alias_map with the exact header.\"\n",
    "    )\n",
    "\n",
    "# Rename to canonical names and coerce numeric\n",
    "df = df.rename(columns={v: k for k, v in chosen.items()})\n",
    "for c in required:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with any NaNs in the used columns\n",
    "df = df[required].dropna().reset_index(drop=True)\n",
    "\n",
    "# Feature/target split\n",
    "X = df[[\"Zh\", \"Zdr\", \"Ldr\", \"Kdp\", \"Ah\", \"Adp\"]].copy()\n",
    "y = df[\"R\"].astype(float).values\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "\n",
    "\n",
    "# ----------------------------- Train/Test Split -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "# ----------------------------- Baseline (Z–R) -----------------------------\n",
    "yhat_train_base = baseline_R_from_Zh_dBZ(X_train[\"Zh\"])\n",
    "yhat_test_base  = baseline_R_from_Zh_dBZ(X_test[\"Zh\"])\n",
    "\n",
    "results = []\n",
    "results.append(report(\"Baseline Z–R\", y_train, yhat_train_base, y_test, yhat_test_base))\n",
    "\n",
    "\n",
    "# ----------------------------- Linear Regression -----------------------------\n",
    "linreg = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),      # optional but consistent across models\n",
    "    (\"lr\", LinearRegression())\n",
    "])\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "yhat_train_lr = linreg.predict(X_train)\n",
    "yhat_test_lr  = linreg.predict(X_test)\n",
    "results.append(report(\"Linear Regression\", y_train, yhat_train_lr, y_test, yhat_test_lr))\n",
    "\n",
    "\n",
    "# ----------------------------- Polynomial Regression (0–9), 7-fold CV -----------------------------\n",
    "from joblib import Memory\n",
    "cache_dir = Path(\"homework/.sk_cache\"); cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "memory = Memory(cache_dir, verbose=0)\n",
    "\n",
    "# cast to float32 to reduce memory pressure\n",
    "X_train32 = X_train.astype(np.float32)\n",
    "X_test32  = X_test.astype(np.float32)\n",
    "y_train32 = y_train.astype(np.float32)\n",
    "y_test32  = y_test.astype(np.float32)\n",
    "\n",
    "poly_pipe = Pipeline(steps=[\n",
    "    (\"poly\", PolynomialFeatures(include_bias=True)),\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "    (\"lr\", LinearRegression())\n",
    "], memory=memory)\n",
    "\n",
    "cv = KFold(n_splits=7, shuffle=True, random_state=42)\n",
    "param_grid_poly = {\"poly__degree\": list(range(0, 10))}\n",
    "\n",
    "grid_poly = GridSearchCV(\n",
    "    estimator=poly_pipe,\n",
    "    param_grid=param_grid_poly,\n",
    "    scoring=\"r2\",\n",
    "    cv=cv,\n",
    "    n_jobs=1,      # <-- single-core to avoid Windows worker crashes\n",
    "    refit=True,\n",
    "    verbose=1,\n",
    "    pre_dispatch=\"1*n_jobs\",\n",
    "    return_train_score=False,\n",
    ")\n",
    "\n",
    "grid_poly.fit(X_train32, y_train32)\n",
    "\n",
    "best_poly = grid_poly.best_estimator_\n",
    "best_deg = grid_poly.best_params_[\"poly__degree\"]\n",
    "\n",
    "yhat_train_poly = best_poly.predict(X_train32)\n",
    "yhat_test_poly  = best_poly.predict(X_test32)\n",
    "results.append(report(f\"Polynomial (deg={best_deg})\", y_train32, yhat_train_poly, y_test32, yhat_test_poly))\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------- Random Forest (tuned), 7-fold CV (Windows-friendly) -----------------------------\n",
    "# Use single-core everywhere to avoid Windows pickling + paging-file errors\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=1)  # single-core inside each RF\n",
    "\n",
    "param_grid_rf = {\n",
    "    \"bootstrap\": [True, False],\n",
    "    \"max_depth\": [10, 100],\n",
    "    \"max_features\": [\"sqrt\", 1.0],\n",
    "    \"min_samples_leaf\": [1, 4],\n",
    "    \"min_samples_split\": [2, 10],\n",
    "    \"n_estimators\": [200, 1000],\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid_rf,\n",
    "    scoring=\"r2\",\n",
    "    cv=cv,            # 7-fold CV as specified\n",
    "    n_jobs=1,         # single-core grid search to avoid loky issues on Windows\n",
    "    refit=True,\n",
    "    verbose=1,\n",
    "    pre_dispatch=\"1*n_jobs\",\n",
    "    return_train_score=False,\n",
    ")\n",
    "\n",
    "# With n_jobs=1, this is effectively serial; using threading backend is harmless\n",
    "with parallel_backend(\"threading\", n_jobs=1):\n",
    "    grid_rf.fit(X_train, y_train)\n",
    "\n",
    "best_rf = grid_rf.best_estimator_\n",
    "\n",
    "yhat_train_rf = best_rf.predict(X_train)\n",
    "yhat_test_rf  = best_rf.predict(X_test)\n",
    "results.append(report(\"Random Forest (tuned)\", y_train, yhat_train_rf, y_test, yhat_test_rf))\n",
    "\n",
    "\n",
    "# ----------------------------- Summary -----------------------------\n",
    "print(\"\\n=== Summary ===\")\n",
    "summary = pd.DataFrame(results)\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "# Save for your README\n",
    "out_path = Path(\"homework/module5_results_summary.csv\")\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "summary.to_csv(out_path, index=False)\n",
    "\n",
    "# Optional: print best model details\n",
    "print(\"\\nBest Polynomial degree:\", best_deg)\n",
    "print(\"Best RF params:\", grid_rf.best_params_)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xarray-climate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
